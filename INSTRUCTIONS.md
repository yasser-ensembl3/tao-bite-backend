# üìñ Instructions compl√®tes - Tao Bite Backend

Guide complet pour installer, lancer et utiliser l'API backend.

---

## üöÄ Installation et Lancement

### Pr√©requis
- Python 3.9+
- pip3
- git

### √âtape 1 : Cloner le repository

```bash
git clone https://github.com/yasser-ensembl3/tao-bite-backend.git
cd tao-bite-backend
```

### √âtape 2 : Cr√©er un environnement virtuel (recommand√©)

```bash
# macOS/Linux
python3 -m venv venv
source venv/bin/activate

# Windows
python3 -m venv venv
venv\Scripts\activate
```

### √âtape 3 : Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### √âtape 4 : Configurer les cl√©s API

Cr√©ez un fichier `.env` √† la racine du projet :

```bash
cp .env.example .env
```

√âditez le fichier `.env` avec vos cl√©s API :

```env
# API Keys Configuration
LLAMA_CLOUD_API_KEY=votre_cle_llama_cloud
OPENAI_API_KEY=votre_cle_openai
ANTHROPIC_API_KEY=votre_cle_anthropic

# Qdrant Cloud Configuration (optionnel)
QDRANT_URL=votre_url_qdrant_cloud
QDRANT_API_KEY=votre_cle_qdrant
```

### √âtape 5 : Lancer le serveur

```bash
python3 app.py
```

Le serveur d√©marre sur **http://localhost:8080**

---

## üìö Utilisation de l'API

### Workflow complet

#### 1Ô∏è‚É£ Uploader un PDF

**Commande :**
```bash
curl -X POST http://localhost:8080/upload \
  -F "file=@chemin/vers/votre-document.pdf"
```

**Exemple avec un fichier test :**
```bash
curl -X POST http://localhost:8080/upload \
  -F "file=@test.pdf"
```

**R√©ponse attendue :**
```json
{
  "success": true,
  "message": "File uploaded successfully",
  "job_id": "abc123-def456-789ghi",
  "filename": "test.pdf"
}
```

**üí° Important :** Notez le `job_id` - vous en aurez besoin pour les √©tapes suivantes !

---

#### 2Ô∏è‚É£ V√©rifier le statut de conversion

**Commande :**
```bash
curl http://localhost:8080/status/VOTRE_JOB_ID
```

**Exemple :**
```bash
curl http://localhost:8080/status/abc123-def456-789ghi
```

**R√©ponse (en cours) :**
```json
{
  "status": "processing",
  "message": "Converting PDF...",
  "job_id": "abc123-def456-789ghi"
}
```

**R√©ponse (termin√©e) :**
```json
{
  "status": "completed",
  "message": "Conversion complete",
  "job_id": "abc123-def456-789ghi",
  "markdown_file": "outputs/abc123-def456-789ghi.md"
}
```

**üí° Astuce :** Attendez que le statut soit "completed" avant de passer √† l'√©tape suivante.

---

#### 3Ô∏è‚É£ T√©l√©charger le markdown (optionnel)

**Commande :**
```bash
curl http://localhost:8080/download/VOTRE_JOB_ID -o document.md
```

**Exemple :**
```bash
curl http://localhost:8080/download/abc123-def456-789ghi -o mon-document.md
```

---

#### 4Ô∏è‚É£ Chunking + Embeddings + Injection dans la base vectorielle

Cette commande fait tout automatiquement :
- D√©coupe le texte en chunks
- G√©n√®re les embeddings avec OpenAI
- Injecte dans Qdrant

**Commande :**
```bash
curl -X POST http://localhost:8080/auto-pipeline/VOTRE_JOB_ID \
  -H "Content-Type: application/json" \
  -d '{
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "collection_name": "pdf_documents"
  }'
```

**Exemple :**
```bash
curl -X POST http://localhost:8080/auto-pipeline/abc123-def456-789ghi \
  -H "Content-Type: application/json" \
  -d '{
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "collection_name": "pdf_documents"
  }'
```

**Param√®tres :**
- `chunk_size` : Taille de chaque chunk en tokens (recommand√©: 1000)
- `chunk_overlap` : Chevauchement entre chunks (recommand√©: 200)
- `collection_name` : Nom de la collection Qdrant (d√©faut: "pdf_documents")

**R√©ponse attendue :**
```json
{
  "success": true,
  "message": "Pipeline completed successfully",
  "total_chunks": 145,
  "total_tokens": 98432,
  "collection_name": "pdf_documents"
}
```

---

#### 5Ô∏è‚É£ G√©n√©rer du contenu avec Claude AI

Recherchez s√©mantiquement dans vos documents et g√©n√©rez du contenu avec Claude.

**Commande :**
```bash
curl -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "vos mots-cl√©s",
    "instructions": "ce que vous voulez g√©n√©rer",
    "num_chunks": 10,
    "min_relevance": 0.3
  }'
```

**Exemples pratiques :**

**Exemple 1 : Extraire des citations**
```bash
curl -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "entrepreneurship leadership",
    "instructions": "Extraire les 5 meilleures citations avec les noms des auteurs",
    "num_chunks": 10
  }'
```

**Exemple 2 : R√©sumer des concepts**
```bash
curl -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "innovation startup",
    "instructions": "R√©sumer les concepts cl√©s en 5 points principaux",
    "num_chunks": 15
  }'
```

**Exemple 3 : Cr√©er un article**
```bash
curl -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "decision making psychology",
    "instructions": "Cr√©er un article de blog de 500 mots sur ce sujet",
    "num_chunks": 20
  }'
```

**Param√®tres :**
- `keywords` (obligatoire) : Mots-cl√©s pour la recherche s√©mantique
- `instructions` (obligatoire) : Instructions pour Claude AI
- `num_chunks` (optionnel) : Nombre de passages pertinents √† utiliser (d√©faut: 10)
- `min_relevance` (optionnel) : Score minimum de pertinence 0-1 (d√©faut: 0.3)

**R√©ponse :**
```json
{
  "success": true,
  "content": "Le contenu g√©n√©r√© par Claude...",
  "metadata": {
    "chunks_found": 10,
    "avg_relevance": 0.72,
    "max_relevance": 0.89,
    "processing_time": 2.34
  }
}
```

---

## üìä Consulter la base de donn√©es

### Voir les statistiques de la base

```bash
curl http://localhost:8080/api/database/stats
```

**R√©ponse :**
```json
{
  "collections": [
    {
      "name": "pdf_documents",
      "vectors_count": 2335,
      "vector_size": 1536
    }
  ],
  "total_vectors": 2335
}
```

---

### Lister tous les documents

```bash
curl http://localhost:8080/api/database/documents/list
```

**R√©ponse :**
```json
{
  "collection_name": "pdf_documents",
  "documents": [
    {
      "filename": "Thinking Fast and Slow.pdf",
      "chunk_count": 410,
      "total_tokens": 265038,
      "source": "pdfplumber",
      "job_id": "abc123"
    },
    {
      "filename": "Zero to One.pdf",
      "chunk_count": 285,
      "total_tokens": 189432,
      "source": "llamaparse",
      "job_id": "def456"
    }
  ],
  "total_documents": 2,
  "total_chunks": 695
}
```

---

### Rechercher un document sp√©cifique

```bash
curl "http://localhost:8080/api/database/documents/list?search=thinking"
```

---

### Voir les documents avec pagination

```bash
curl "http://localhost:8080/api/database/documents?limit=50&offset=0"
```

---

## üîç Recherche s√©mantique dans Qdrant

```bash
curl -X POST http://localhost:8080/qdrant/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "votre recherche",
    "collection_name": "pdf_documents",
    "limit": 10
  }'
```

---

## ü§ñ Script automatis√© complet

Cr√©ez un fichier `test-pipeline.sh` :

```bash
#!/bin/bash

# Configuration
PDF_FILE="mon-document.pdf"
COLLECTION="pdf_documents"

echo "=========================================="
echo "üöÄ PIPELINE COMPLET TAO BITE BACKEND"
echo "=========================================="

# 1. Upload
echo ""
echo "üì§ √âtape 1/5 : Upload du PDF..."
RESPONSE=$(curl -s -X POST http://localhost:8080/upload -F "file=@$PDF_FILE")
JOB_ID=$(echo $RESPONSE | jq -r '.job_id')

if [ "$JOB_ID" == "null" ]; then
  echo "‚ùå Erreur lors de l'upload"
  echo $RESPONSE | jq '.'
  exit 1
fi

echo "‚úÖ Upload r√©ussi - Job ID: $JOB_ID"

# 2. Attendre la conversion
echo ""
echo "‚è≥ √âtape 2/5 : Conversion en cours..."
while true; do
  STATUS=$(curl -s http://localhost:8080/status/$JOB_ID | jq -r '.status')

  if [ "$STATUS" == "completed" ]; then
    echo "‚úÖ Conversion termin√©e!"
    break
  elif [ "$STATUS" == "error" ]; then
    echo "‚ùå Erreur lors de la conversion"
    exit 1
  fi

  echo "   Status: $STATUS - attente..."
  sleep 2
done

# 3. Processing + Injection
echo ""
echo "üîÑ √âtape 3/5 : Chunking et injection dans Qdrant..."
PIPELINE_RESPONSE=$(curl -s -X POST http://localhost:8080/auto-pipeline/$JOB_ID \
  -H "Content-Type: application/json" \
  -d "{
    \"chunk_size\": 1000,
    \"chunk_overlap\": 200,
    \"collection_name\": \"$COLLECTION\"
  }")

TOTAL_CHUNKS=$(echo $PIPELINE_RESPONSE | jq -r '.total_chunks')
echo "‚úÖ Pipeline termin√© - $TOTAL_CHUNKS chunks cr√©√©s"

# 4. Stats
echo ""
echo "üìä √âtape 4/5 : Statistiques de la base..."
curl -s http://localhost:8080/api/database/stats | jq '.'

# 5. G√©n√©ration de contenu
echo ""
echo "ü§ñ √âtape 5/5 : G√©n√©ration de contenu IA..."
CONTENT=$(curl -s -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "innovation startup entrepreneurship",
    "instructions": "R√©sumer les concepts cl√©s en 3 points principaux",
    "num_chunks": 10
  }')

echo ""
echo "=========================================="
echo "‚úÖ CONTENU G√âN√âR√â :"
echo "=========================================="
echo $CONTENT | jq -r '.content'
echo ""
echo "=========================================="
echo "üìà M√©tadonn√©es :"
echo "=========================================="
echo $CONTENT | jq '.metadata'

echo ""
echo "‚úÖ Pipeline complet termin√© avec succ√®s!"
```

**Utilisation :**
```bash
chmod +x test-pipeline.sh
./test-pipeline.sh
```

**Note :** Ce script n√©cessite `jq` pour parser le JSON.
```bash
# macOS
brew install jq

# Ubuntu/Debian
sudo apt-get install jq
```

---

## üõ†Ô∏è Commandes utiles

### Arr√™ter le serveur
```bash
# Dans le terminal o√π le serveur tourne
Ctrl+C
```

### Red√©marrer le serveur
```bash
python3 app.py
```

### V√©rifier si le serveur fonctionne
```bash
curl http://localhost:8080/api/database/stats
```

### Nettoyer les uploads et outputs
```bash
rm -rf uploads/* outputs/*
```

### Voir les logs en temps r√©el
Les logs s'affichent directement dans le terminal avec des emojis :
- üîç = Recherche/Requ√™te
- ‚úì = Succ√®s
- ‚ùå = Erreur
- üìö = Base de donn√©es
- üéØ = Configuration

---

## üêõ D√©pannage

### Le serveur ne d√©marre pas

**Erreur : Port 8080 d√©j√† utilis√©**
```bash
# Trouver le processus
lsof -ti:8080

# Tuer le processus
kill -9 $(lsof -ti:8080)
```

**Erreur : Module manquant**
```bash
pip install -r requirements.txt
```

---

### Les cl√©s API ne fonctionnent pas

1. V√©rifiez que le fichier `.env` existe
2. V√©rifiez que les cl√©s sont correctes (sans espaces)
3. Red√©marrez le serveur apr√®s modification du `.env`

---

### La conversion √©choue

Le syst√®me a 2 m√©thodes de fallback :
1. **pdfplumber** (rapide, pour PDFs simples)
2. **LlamaParse** (backup, pour PDFs complexes)

Si les deux √©chouent, v√©rifiez :
- Le PDF n'est pas corrompu
- Le PDF n'est pas prot√©g√© par mot de passe
- Votre cl√© `LLAMA_CLOUD_API_KEY` est valide

---

### Qdrant ne fonctionne pas

**Option 1 : Utiliser Qdrant local**
- Ne d√©finissez pas `QDRANT_URL` et `QDRANT_API_KEY` dans `.env`
- Les donn√©es seront stock√©es dans `./qdrant_storage/`

**Option 2 : Utiliser Qdrant Cloud**
- V√©rifiez que `QDRANT_URL` et `QDRANT_API_KEY` sont corrects
- Format URL : `https://xxx.cloud.qdrant.io`

---

## üìñ Ressources

- **API compl√®te** : Voir `API.md`
- **Guide rapide** : Voir `QUICKSTART.md`
- **README** : Voir `README.md`
- **GitHub** : https://github.com/yasser-ensembl3/tao-bite-backend

---

## üí° Exemples d'utilisation

### Cas d'usage 1 : Biblioth√®que de livres

Upload plusieurs livres et posez des questions cross-documents :

```bash
# Upload livre 1
curl -X POST http://localhost:8080/upload -F "file=@livre1.pdf"
# Attendez conversion + auto-pipeline

# Upload livre 2
curl -X POST http://localhost:8080/upload -F "file=@livre2.pdf"
# Attendez conversion + auto-pipeline

# Recherche cross-documents
curl -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "leadership resilience",
    "instructions": "Comparer les perspectives des diff√©rents auteurs sur ce sujet",
    "num_chunks": 20
  }'
```

---

### Cas d'usage 2 : Extraction de citations

```bash
curl -X POST http://localhost:8080/generate-content \
  -H "Content-Type: application/json" \
  -d '{
    "keywords": "failure success pivot",
    "instructions": "Extraire 10 citations inspirantes sur l'\''√©chec et le pivot, avec nom de l'\''auteur et contexte",
    "num_chunks": 15
  }'
```

---

### Cas d'usage 3 : G√©n√©ration de contenu Substack

```bash
curl -X POST http://localhost:8080/generate-draft \
  -H "Content-Type: application/json" \
  -d '{
    "topic": "The Art of Decision Making",
    "num_chunks": 20,
    "style": "conversational"
  }'
```

---

## üîê S√©curit√© (Production)

Si vous d√©ployez en production :

1. **D√©sactivez le mode debug** dans `app.py`
   ```python
   app.run(host='0.0.0.0', port=8080, debug=False)
   ```

2. **Utilisez un serveur WSGI** (Gunicorn ou Waitress)
   ```bash
   gunicorn -w 4 -b 0.0.0.0:8080 app:app
   ```

3. **Configurez CORS** pour votre domaine uniquement

4. **Utilisez HTTPS** (nginx + Let's Encrypt)

5. **Ajoutez une authentification** (JWT, API keys, etc.)

6. **Rate limiting** (Flask-Limiter ou nginx)

---

## ‚úÖ Checklist avant de commencer

- [ ] Python 3.9+ install√©
- [ ] pip install√©
- [ ] Repository clon√©
- [ ] D√©pendances install√©es
- [ ] Fichier `.env` cr√©√© avec les cl√©s API
- [ ] Serveur lanc√© et accessible sur http://localhost:8080
- [ ] Test de base r√©ussi (`curl http://localhost:8080/api/database/stats`)

---

**Vous √™tes pr√™t √† utiliser Tao Bite Backend ! üöÄ**
